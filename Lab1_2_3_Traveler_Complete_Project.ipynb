{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of Data Science - Traveler\n",
    "\n",
    "### * Goal: Predict the country that users will make their first booking in, based on some basic user profile data.\n",
    "\n",
    "\n",
    "#### [1] Pre-processing: Assessing and analyzing data, cleaning, transforming and adding new features\n",
    "#### [2] Learning model: Constructing and testing learning model\n",
    "#### [3] Post-processing: Creating final predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 1 CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Exploring Traveler data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%pylab inline \n",
    "\n",
    "print(\"Reading data...\")\n",
    "train_file = \"./traveler/train_users_2.csv\"\n",
    "df_train = pd.read_csv(train_file, header = 0,index_col=None)\n",
    "\n",
    "test_file = \"./traveler/test_users.csv\"\n",
    "df_test = pd.read_csv(test_file, header = 0,index_col=None)\n",
    "\n",
    "# Combining into one dataset for cleaning\n",
    "df_all = pd.concat((df_train, df_test), axis=0, ignore_index=True)\n",
    "print(\"Reading data...completed\")\n",
    "\n",
    "# Fixing date formats in Pandas - to_datetime\n",
    "## Change dates to specific format\n",
    "print(\"Fixing timestamps...\")\n",
    "df_all['date_account_created'] = pd.to_datetime(df_all['date_account_created'], format='%Y-%m-%d')\n",
    "df_all['timestamp_first_active'] = pd.to_datetime(df_all['timestamp_first_active'], format='%Y%m%d%H%M%S')\n",
    "print(\"Fixing timestamps...completed\")\n",
    "\n",
    "## Removing date_first_booking column\n",
    "df_all.drop('date_first_booking', axis = 1, inplace = True)\n",
    "print(\"Droped date_first_booking column...\")\n",
    "\n",
    "## Remove outliers function - [1]\n",
    "def remove_outliers(df, column, min_val, max_val):\n",
    "    col_values = df[column].values\n",
    "    df[column] = np.where(np.logical_or(col_values<=min_val, col_values>=max_val), np.NaN, col_values)\n",
    "    return df\n",
    "\n",
    "## Fixing age column - [2]\n",
    "print(\"Fixing age column...\")\n",
    "df_all = remove_outliers(df = df_all, column = 'age', min_val = 15, max_val = 90)\n",
    "df_all['age'].fillna(-1, inplace = True)\n",
    "print(\"Fixing age column...completed\")\n",
    "\n",
    "# Other column missing value - Fill first_affiliate_tracked column\n",
    "print(\"Filling first_affiliate_tracked column...\")\n",
    "df_all['first_affiliate_tracked'].fillna(-1, inplace=True)\n",
    "print(\"Filling first_affiliate_tracked column...completed\")\n",
    "\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 2 CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Own implementation of One Hot Encoding - Data Transformation\n",
    "def convert_to_binary(df, column_to_convert):\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert[:5] + '_' + cat_name[:10]\n",
    "        df[col_name] = 0\n",
    "        df.loc[(df[column_to_convert] == category), col_name] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# One Hot Encoding\n",
    "print(\"One Hot Encoding categorical data...\")\n",
    "columns_to_convert = ['gender', 'signup_method', 'signup_flow', 'language', 'affiliate_channel', 'affiliate_provider', 'first_affiliate_tracked', 'signup_app', 'first_device_type', 'first_browser']\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    df_all = convert_to_binary(df=df_all, column_to_convert=column)\n",
    "    df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"One Hot Encoding categorical data...completed\")\n",
    "\n",
    "# Add new date related fields - Creating New Features\n",
    "print(\"Adding new fields...\")\n",
    "df_all['day_account_created'] = df_all['date_account_created'].dt.weekday\n",
    "df_all['month_account_created'] = df_all['date_account_created'].dt.month\n",
    "df_all['quarter_account_created'] = df_all['date_account_created'].dt.quarter\n",
    "df_all['year_account_created'] = df_all['date_account_created'].dt.year\n",
    "df_all['hour_first_active'] = df_all['timestamp_first_active'].dt.hour\n",
    "df_all['day_first_active'] = df_all['timestamp_first_active'].dt.weekday\n",
    "df_all['month_first_active'] = df_all['timestamp_first_active'].dt.month\n",
    "df_all['quarter_first_active'] = df_all['timestamp_first_active'].dt.quarter\n",
    "df_all['year_first_active'] = df_all['timestamp_first_active'].dt.year\n",
    "df_all['created_less_active'] = (df_all['date_account_created'] - df_all['timestamp_first_active']).dt.days\n",
    "print(\"Adding new fields...completed\")\n",
    "\n",
    "\n",
    "# Drop unnecessary columns\n",
    "print(\"Droping fields...\")\n",
    "columns_to_drop = ['date_account_created', 'timestamp_first_active', 'date_first_booking', 'country_destination']\n",
    "for column in columns_to_drop:\n",
    "    if column in df_all.columns:\n",
    "        df_all.drop(column, axis=1, inplace=True)\n",
    "print(\"Droping fields...completed\")\n",
    "\n",
    "## Understanding the sessions.csv data\n",
    "## Loading sessions.csv data\n",
    "print(\"Reading sessions data...\")\n",
    "sessions_file = \"./traveler/sessions.csv\"\n",
    "df_sessions = pd.read_csv(sessions_file, header = 0,index_col=False)\n",
    "print(\"Reading sessions data...completed\")\n",
    "\n",
    "## Cleaning and Transforming the Data\n",
    "# Determine primary device\n",
    "print(\"Determing primary device...\")\n",
    "sessions_device = df_sessions.loc[:, ['user_id', 'device_type', 'secs_elapsed']]\n",
    "aggregated_lvl1 = sessions_device.groupby(['user_id', 'device_type'], as_index=False, sort=False).aggregate(np.sum)\n",
    "#aggregated_lvl1.head(10)\n",
    "idx = aggregated_lvl1.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == aggregated_lvl1['secs_elapsed']\n",
    "#idx.head(10)\n",
    "df_sessions_primary = pd.DataFrame(aggregated_lvl1.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "#df_sessions_primary.head(10)\n",
    "df_sessions_primary.rename(columns = {'device_type':'primary_device', 'secs_elapsed':'primary_secs'}, inplace=True)\n",
    "#df_sessions_primary.head(10)\n",
    "# Call user defined One Hot Encoding function\n",
    "df_sessions_primary = convert_to_binary(df=df_sessions_primary, column_to_convert='primary_device')\n",
    "#df_sessions_primary.head()\n",
    "df_sessions_primary.drop('primary_device', axis=1, inplace=True)\n",
    "#df_sessions_primary.head()\n",
    "print(\"Determing primary device...completed\")\n",
    "\n",
    "# Determine Secondary device\n",
    "print(\"Determing secondary device...\")\n",
    "remaining = aggregated_lvl1.drop(aggregated_lvl1.index[idx])\n",
    "remaining.head()\n",
    "idx = remaining.groupby(['user_id'], sort=False)['secs_elapsed'].transform(max) == remaining['secs_elapsed']\n",
    "df_sessions_secondary = pd.DataFrame(remaining.loc[idx , ['user_id', 'device_type', 'secs_elapsed']])\n",
    "df_sessions_secondary.rename(columns = {'device_type':'secondary_device', 'secs_elapsed':'secondary_secs'}, inplace=True)\n",
    "df_sessions_secondary = convert_to_binary(df=df_sessions_secondary, column_to_convert='secondary_device')\n",
    "df_sessions_secondary.drop('secondary_device', axis=1, inplace=True)\n",
    "print(\"Determing secondary device...completed\")\n",
    "\n",
    "# Determine Counts of Actions - Looping Through the Actions Columns\n",
    "# Count occurrences of value in a column\n",
    "def convert_to_counts(df, id_col, column_to_convert):\n",
    "    id_list = df[id_col].drop_duplicates()\n",
    "\n",
    "    df_counts = df.loc[:,[id_col, column_to_convert]]\n",
    "    df_counts['count'] = 1\n",
    "    df_counts = df_counts.groupby(by=[id_col, column_to_convert], as_index=False, sort=False).sum()\n",
    "\n",
    "    new_df = df_counts.pivot(index=id_col, columns=column_to_convert, values='count')\n",
    "    new_df = new_df.fillna(0)\n",
    "\n",
    "# Rename Columns\n",
    "    categories = list(df[column_to_convert].drop_duplicates())\n",
    "    for category in categories:\n",
    "        cat_name = str(category).replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\").replace(\"-\", \"\").lower()\n",
    "        col_name = column_to_convert + '_' + cat_name\n",
    "        new_df.rename(columns = {category:col_name}, inplace=True)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "# Aggregate and combine actions taken columns\n",
    "print(\"Aggregating actions taken...\")\n",
    "session_actions = df_sessions.loc[:,['user_id', 'action', 'action_type', 'action_detail']]\n",
    "columns_to_convert = ['action', 'action_type', 'action_detail']\n",
    "session_actions = session_actions.fillna('not provided')\n",
    "first = True\n",
    "\n",
    "for column in columns_to_convert:\n",
    "    print(\"Converting \" + column + \" column...\")\n",
    "    current_data = convert_to_counts(df=session_actions, id_col='user_id', column_to_convert=column)\n",
    "\n",
    "# If first loop, current data becomes existing data, otherwise merge existing and current\n",
    "    if first:\n",
    "        first = False\n",
    "        actions_data = current_data\n",
    "    else:\n",
    "        actions_data = pd.concat([actions_data, current_data], axis=1, join='inner')\n",
    "\n",
    "# Finally, Combine Data Sets\n",
    "# [4.1] Merge device datasets - First, combine the two device dataframes (df_primary and df_secondary) to create a device dataframe.\n",
    "print(\"Combining results...\")\n",
    "df_sessions_primary.set_index('user_id', inplace=True)\n",
    "df_sessions_secondary.set_index('user_id', inplace=True)\n",
    "device_data = pd.concat([df_sessions_primary, df_sessions_secondary], axis=1, join=\"outer\")\n",
    "\n",
    "# [4.2] Merge device and actions datasets - Then, combine the device dataframe with the actions dataframe to create a sessions dataframe with all the features extracted from sessions.csv\n",
    "combined_results = pd.concat([device_data, actions_data], axis=1, join='outer')\n",
    "df_sessions_complete = combined_results.fillna(0)\n",
    "\n",
    "# [4.3] Merge user and session datasets - Finally, combine the sessions dataframe with the user data dataframe computed earlier\n",
    "df_all.set_index('id', inplace=True)\n",
    "df_all = pd.concat([df_all, df_sessions_complete], axis=1, join='inner')\n",
    "print(\"Combining results...completed\")\n",
    "\n",
    "df_all.head() # You need get 5 rows Ã— 720 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LAB 3 CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Temp variables created to store data\n",
    "df_train1 = df_train\n",
    "df_test1 = df_test\n",
    "df_all1 = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating a learning model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df_train1.set_index('id', inplace=True)\n",
    "df_train1 = pd.concat([df_train1['country_destination'], \n",
    "                       df_all1], axis=1, join='inner')\n",
    "\n",
    "id_train = df_train1.index.values\n",
    "labels = df_train1['country_destination']\n",
    "\n",
    "# Label encoding for the categorical data eg: ...NDF -> 7, US -> 10...\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "X = df_train1.drop('country_destination', axis=1, inplace=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1 to build and test learning model\n",
    "##### Cross-Validation - Training data split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn import cross_validation\n",
    "## Spliting of training dataset into 70% training data and 30% testing data randomly\n",
    "features_train, features_test, labels_train, labels_test = \n",
    "cross_validation.train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Different classification techniques\n",
    "## Decision Tree \n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "### Gaussian Naive Bayes\n",
    "### from sklearn.naive_bayes import GaussianNB\n",
    "### clf = GaussianNB()\n",
    "\n",
    "##SVM\n",
    "##from sklearn import svm\n",
    "##clf = svm.SVC(kernel=\"rbf\") \n",
    "clf.fit(features_train, labels_train)\n",
    "prediction = clf.predict(features_test)\n",
    "## Computing accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print accuracy_score(prediction, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2 to build and test learning model\n",
    "##### Cross-Validation - Training data split into training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn import decomposition, grid_search\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss\n",
    "\n",
    "# Grid Search - Used to find best combination of parameters\n",
    "XGB_model = xgb.XGBClassifier(objective='multi:softprob', subsample=0.5, colsample_bytree=0.5, seed=0)\n",
    "param_grid = {'max_depth': [5], 'learning_rate': [0.1], 'n_estimators': [5]}\n",
    "\n",
    "##Note running this step can take a significant amount of time, might take hours as well.\n",
    "#param_grid = {'max_depth': [3, 4, 5], 'learning_rate': [0.1, 0.3], 'n_estimators': [25, 50]} \n",
    "\n",
    "model = grid_search.GridSearchCV(estimator=XGB_model, param_grid=param_grid, scoring='accuracy', verbose=10, n_jobs=1, iid=True, refit=True, cv=3)\n",
    "\n",
    "# The actual model for complete training data\n",
    "#model.fit(X, y)\n",
    "model.fit(features_train, labels_train)\n",
    "print(\"Best score: %0.3f\" % model.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = model.best_estimator_.get_params()\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(features_test)\n",
    "#y_gb = model.predict_proba(features_test)\n",
    "#y_pred_prob = model.predict_proba(features_test) ##select the 5 best predictions\n",
    "\n",
    "#Print model report:\n",
    "print \"\\nModel Report\"\n",
    "print \"Accuracy : %.4g\" % accuracy_score(labels_test, y_pred)\n",
    "#print \"Log loss : %.4g\" % log_loss(labels_test, y_gb)\n",
    "#print \"AUC Score (Train): %f\" % roc_auc_score(labels_test, y_pred_prob)\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 3 to build and test learning model\n",
    "##### Cross-Validation - Training data split into training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#fixing random state\n",
    "random_state=1\n",
    "#Spliting data into train and test sets.\n",
    "XA, X_testA, yA, y_testA = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "#Spliting train data into training and validation sets.\n",
    "X_trainA, X_validA, y_trainA, y_validA = train_test_split(XA, yA, test_size=0.25, random_state=random_state)\n",
    "\n",
    "print('Data shape:')\n",
    "print('X_trainA: %s, X_validA: %s, X_testA: %s \\n' %(X_trainA.shape, X_validA.shape, X_testA.shape))\n",
    "\n",
    "#Defining the classifiers\n",
    "clfs = {'LR'  : LogisticRegression(random_state=random_state), \n",
    "        'SVM' : SVC(probability=True, random_state=random_state), \n",
    "        'RF'  : RandomForestClassifier(n_estimators=100, n_jobs=-1, \n",
    "                                       random_state=random_state), \n",
    "        'GBM' : GradientBoostingClassifier(n_estimators=50, \n",
    "                                           random_state=random_state), \n",
    "        'ETC' : ExtraTreesClassifier(n_estimators=100, n_jobs=-1, \n",
    "                                     random_state=random_state),\n",
    "        'KNN' : KNeighborsClassifier(n_neighbors=30)}\n",
    "    \n",
    "#predictions on the validation and test sets\n",
    "p_valid = []\n",
    "p_test = []\n",
    "   \n",
    "print('Performance of individual classifiers on X_testA')   \n",
    "print('------------------------------------------------------------')\n",
    "   \n",
    "for nm, clf in clfs.items():\n",
    "    #First run. Training on (X_trainA, y_trainA) and predicting on X_validA.\n",
    "    clf.fit(X_trainA, y_trainA)\n",
    "    yv = clf.predict_proba(X_validA)\n",
    "    p_valid.append(yv)\n",
    "        \n",
    "    #Second run. Training on (XA, yA) and predicting on X_testA.\n",
    "    clf.fit(XA, yA)\n",
    "    yt = clf.predict_proba(X_testA)\n",
    "    p_test.append(yt)\n",
    "       \n",
    "    #Printing out the performance of the classifier\n",
    "    print('{:10s} {:2s} {:1.7f}'.format('%s: ' %(nm), 'logloss  =>', log_loss(y_testA, yt)))\n",
    "print('')\n",
    "\n",
    "#Creating the data for the 2nd layer.\n",
    "XV = np.hstack(p_valid)\n",
    "XT = np.hstack(p_test)\n",
    "\n",
    "#By default the best C parameter is obtained with a cross-validation approach, doing grid search with\n",
    "#10 values defined in a logarithmic scale between 1e-4 and 1e4.\n",
    "#Change parameters to see how they affect the final results.\n",
    "lr = LogisticRegressionCV(Cs=10, dual=False, fit_intercept=True, intercept_scaling=1.0, max_iter=25, multi_class='ovr', n_jobs=1, penalty='l2', random_state=random_state, solver='lbfgs', tol=0.0001)\n",
    "\n",
    "lr.fit(XV, y_validA)\n",
    "y_lr = lr.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('Log_Reg:', 'logloss  =>', log_loss(y_testA, y_lr)))\n",
    "\n",
    "#Gradient boosting\n",
    "xgb = XGBClassifier(max_depth=5, learning_rate=0.1, n_estimators=10000, objective='multi:softprob', seed=random_state)\n",
    "xgb.fit(XV, y_validA, early_stopping_rounds=15, verbose=False)\n",
    "xgb.n_estimators = xgb.best_iteration\n",
    "xgb.fit(XV, y_validA)\n",
    "y_gb = xgb.predict_proba(XT)\n",
    "print('{:20s} {:2s} {:1.7f}'.format('XGB_Reg:', 'logloss  =>', log_loss(y_testA, y_gb)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
